{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import accelerate\n",
    "import transformers\n",
    "import huggingface_hub\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"./\"\n",
    "MODEL_DIRECTORY = os.path.join(\".\", \"models\")\n",
    "os.makedirs(MODEL_DIRECTORY, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join(ROOT_PATH, \"datasets\", \"cbp-lkg\", \"legalkg_dataset_prompts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHITESPACE_CHAR_REGEX = re.compile(r\"(?ui)\\s+\")\n",
    "SPECIAL_CHAR_REGEX = re.compile(r\"(?ui)(?:[^\\w\\s]\\s*)*[^\\w\\s]\")\n",
    "\n",
    "def preprocess(entity):\n",
    "    entity = WHITESPACE_CHAR_REGEX.sub(\" \", entity).strip()\n",
    "    entity = SPECIAL_CHAR_REGEX.sub(\"\", entity).strip()\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriplesAsQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, args, max_triples=-1):\n",
    "      self.tokenizer = transformers.AutoTokenizer\\\n",
    "          .from_pretrained(args.base_model)\n",
    "      self._read_data(path, max_triples)\n",
    "\n",
    "    def _read_data(self, path, max_triples):\n",
    "        lines_read, inputs, outputs = 0, [], []\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "          data = json.load(file)\n",
    "          for entry in tqdm(data):\n",
    "            if lines_read == max_triples: break\n",
    "            if len(entry['pair']) != 2:\n",
    "                print(entry)\n",
    "                break\n",
    "            _in, _out = entry['pair']\n",
    "            inputs.append(\"Answer in a few words: \" + _in)\n",
    "            outputs.append(_out)\n",
    "            lines_read += 1\n",
    "        self.data = { 'inputs': inputs, 'outputs': outputs }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['inputs'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data\n",
    "        input  = data['inputs' ][index]\n",
    "        output = data['outputs'][index]\n",
    "        return input, output\n",
    "\n",
    "    def _collate_fn(self, items):\n",
    "        inputs_tokenized  = self.tokenizer(\n",
    "            list(item[0] for item in items),\n",
    "            padding=True, truncation=True,\n",
    "            max_length=128, return_tensors=\"pt\"\n",
    "        )\n",
    "        outputs_tokenized = self.tokenizer(\n",
    "            list(item[1] for item in items),\n",
    "            padding=True, truncation=True,\n",
    "            max_length=32, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids, attention_mask     = inputs_tokenized.input_ids, inputs_tokenized.attention_mask\n",
    "        labels, labels_attention_mask = outputs_tokenized.input_ids, outputs_tokenized.attention_mask\n",
    "        # for labels, set -100 for padding\n",
    "        labels[labels==0] = -100\n",
    "        return input_ids, attention_mask, labels, labels_attention_mask\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return ''.join(self.tokenizer.convert_ids_to_tokens(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    base_model_prefix      = \"google/flan-t5\",\n",
    "    model_size             = \"small\",\n",
    "    model_stem             = \"cbp-lkg-qa\",\n",
    "    epochs                 = 10,\n",
    "    batch_size             = 32,\n",
    "    save_checkpoint        = 5000,\n",
    "    loss_checkpoint        = 500,\n",
    "    num_workers            = 3,\n",
    "    checkpoint             = 0,\n",
    "    max_checkpoints        = 5,\n",
    "    resume_from_checkpoint = True,\n",
    "    gradient_checkpointing = True,\n",
    "    skip_batches           = 0,\n",
    "    finetuning             = False\n",
    ")\n",
    "args.suffix = \"-finetuned\" if args.finetuning else \"\"\n",
    "args.model = \"{0.base_model_prefix}-{0.model_stem}-{0.model_size}{0.suffix}\".format(args)\n",
    "args.model = os.path.basename(args.model)\n",
    "args.model_directory = os.path.join(MODEL_DIRECTORY, args.model)\n",
    "args.base_model = \"{0.base_model_prefix}-{0.model_size}\".format(args)\n",
    "\n",
    "if args.resume_from_checkpoint:\n",
    "  files = glob.glob(os.path.join(args.model_directory, \"chkpt_*.pt\"))\n",
    "  if len(files) > 0:\n",
    "    args.checkpoint = max(int(os.path.basename(file)[6:-3]) for file in files)\n",
    "  else:\n",
    "    args.checkpoint = 0\n",
    "dataset = TriplesAsQADataset(DATASET_PATH, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from genai.client import Client\n",
    "from genai.credentials import Credentials\n",
    "from genai.schema import TextGenerationParameters, TextGenerationReturnOptions\n",
    "from genai.text.generation import CreateExecutionOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "def heading(text: str) -> str:\n",
    "    \"\"\"Helper function for centering text.\"\"\"\n",
    "    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GENAI_KEY=\"\"\n",
    "GENAI_API=\"https://bam-api.res.ibm.com\"\n",
    "\n",
    "credentials = Credentials(api_key=GENAI_KEY, api=GENAI_API)\n",
    "client = Client(credentials=credentials)\n",
    "\n",
    "class QuestionAnsweringModel:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def llm_response_sdk(self, prompt):\n",
    "        try:\n",
    "            parameters = TextGenerationParameters(\n",
    "                max_new_tokens=200,\n",
    "                min_new_tokens=1,\n",
    "                decoding_method=\"greedy\",\n",
    "                return_options=TextGenerationReturnOptions(\n",
    "                    input_text=True,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            response = client.text.generation.create(\n",
    "                model_id=self.model_name,\n",
    "                inputs=[prompt],\n",
    "                parameters=parameters,\n",
    "            )\n",
    "\n",
    "            # Assuming the first result in the batch is what we want\n",
    "            result = next(response).results[0]\n",
    "            generated_text = result.generated_text.split(\"Input\")[0].strip()\n",
    "            return generated_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error during SDK request: {e}\")\n",
    "            return \"Error processing request.\"\n",
    "\n",
    "    def read_dataset(self, file_path) -> List[Dict]:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def generate_prompt(self, triple, question):\n",
    "        return f\"Given the following triples {triple[0]}, {triple[1]}, and {triple[2]}, you need to generate answer for the following question. Give as many answers as possible each separated by a semicolon. Question: {question}\"\n",
    "\n",
    "    def evaluate(self, file_path):\n",
    "        dataset = self.read_dataset(file_path)\n",
    "        hits_at_1 = 0\n",
    "        hits_at_5 = 0\n",
    "        hits_at_10 = 0\n",
    "        total = len(dataset)\n",
    "        \n",
    "        i = 0\n",
    "        for item in dataset:\n",
    "            if i > 10:\n",
    "                break\n",
    "            triple = item[\"triple\"]\n",
    "            question, ground_truth = item[\"pair\"]\n",
    "            prompt = self.generate_prompt(triple, question)\n",
    "            print(prompt)\n",
    "            generated_answers = self.llm_response_sdk(prompt)\n",
    "            print(generated_answers)\n",
    "            \n",
    "            if ground_truth in generated_answers[:1]:\n",
    "                hits_at_1 += 1\n",
    "            if ground_truth in generated_answers[:5]:\n",
    "                hits_at_5 += 1\n",
    "            if ground_truth in generated_answers[:10]:\n",
    "                hits_at_10 += 1\n",
    "\n",
    "        accuracy_at_1 = hits_at_1 / total if total > 0 else 0\n",
    "        accuracy_at_5 = hits_at_5 / total if total > 0 else 0\n",
    "        accuracy_at_10 = hits_at_10 / total if total > 0 else 0\n",
    "\n",
    "        filename = \"./results/metrics.txt\"\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(f\"Hits at 1 Accuracy: {accuracy_at_1*100:.2f}%\\n\")\n",
    "            file.write(f\"Hits at 5 Accuracy: {accuracy_at_5*100:.2f}%\\n\")\n",
    "            file.write(f\"Hits at 10 Accuracy: {accuracy_at_10*100:.2f}%\\n\")\n",
    "\n",
    "model_name = \"codellama/codellama-34b-instruct\"\n",
    "qa_model = QuestionAnsweringModel(model_name)\n",
    "qa_model.evaluate(\"./datasets/cbp-lkg/legalkg_dataset_prompts.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
